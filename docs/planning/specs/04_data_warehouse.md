# 04 - Adatt√°rh√°z (Data Warehouse)

## üéØ C√©l √©s Sz√°nd√©k

Ez a dokumentum defini√°lja a **Neural AI Next** Big Data t√°rol√≥ rendszer√©t, amely 25 √©vnyi Tick adatot k√©pes particion√°lt Parquet form√°tumban t√°rolni √©s gyorsan lek√©rdezni. A rendszer kiz√°r√≥lag a pr√©mium instrumentumokra f√≥kusz√°l: `EURUSD, GBPUSD, USDJPY, USDCHF, XAUUSD`.

**Filoz√≥fia:** *"Fast reads, efficient storage, easy querying"*

---

## üèóÔ∏è Architekt√∫ra √Åttekint√©s

### T√°rol√°si Strat√©gia

```
/data/tick/
‚îú‚îÄ‚îÄ EURUSD/
‚îÇ   ‚îú‚îÄ‚îÄ tick/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2023/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=12/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=01/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet (10-50MB)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=02/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ year=2024/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ GBPUSD/
‚îú‚îÄ‚îÄ USDJPY/
‚îú‚îÄ‚îÄ USDCHF/
‚îî‚îÄ‚îÄ XAUUSD/
```

### Part√≠ci√≥ El≈ëny√∂k

- **Gyors lek√©rdez√©s:** D√°tum √©s szimb√≥lum alap√∫ sz≈±r√©s
- **Hat√©kony t√°rol√°s:** Csak a sz√ºks√©ges adatok bet√∂lt√©se
- **P√°rhuzamos feldolgoz√°s:** T√∂bb part√≠ci√≥ egyszerre feldolgozhat√≥
- **Sk√°l√°zhat√≥s√°g:** √âvek √≥ta gy≈±jt√∂tt adatok kezel√©se

---

## üì¶ Technol√≥giai Stack

### F≈ë F√ºgg≈ës√©gek

```python
# pyproject.toml
dependencies = [
    "fastparquet>=2023.4.0",
    "polars>=0.20.0",  # Gyorsabb mint Pandas
    "pyarrow>=14.0.0",
]
```

### Adatmodell

```python
from pydantic import BaseModel
from datetime import datetime
from typing import Optional

class TickData(BaseModel):
    """Tick adat modell."""
    timestamp: datetime
    symbol: str
    bid: float
    ask: float
    volume: Optional[int] = None
    source: str  # 'jforex', 'mt5', 'ibkr'
    
    @property
    def spread(self) -> float:
        """Spread kisz√°m√≠t√°sa."""
        return self.ask - self.bid
    
    @property
    def mid_price(self) -> float:
        """K√∂z√©p√°r kisz√°m√≠t√°sa."""
        return (self.bid + self.ask) / 2
```

---

## üóÑÔ∏è ParquetStorageService

### Implement√°ci√≥

```python
import polars as pl
from fastparquet import write, ParquetFile
from pathlib import Path
from typing import Optional, List
import asyncio

class ParquetStorageService:
    """Particion√°lt Parquet t√°rol√≥ szolg√°ltat√°s."""
    
    BASE_PATH = Path("/data/tick")
    
    def __init__(self):
        self.engine = "fastparquet"
        self.compression = "snappy"
    
    def _get_path(
        self,
        symbol: str,
        date: datetime
    ) -> Path:
        """El√©r√©si √∫t gener√°l√°sa."""
        return (
            self.BASE_PATH /
            symbol /
            "tick" /
            f"year={date.year}" /
            f"month={date.month:02d}" /
            f"day={date.day:02d}" /
            "data.parquet"
        )
    
    async def store_tick_data(
        self,
        symbol: str,
        data: pl.DataFrame,
        date: datetime
    ) -> None:
        """Tick adatok t√°rol√°sa."""
        path = self._get_path(symbol, date)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Polars DataFrame -> Parquet
        data.write_parquet(
            path,
            compression=self.compression
        )
        
        logger.info(
            "tick_data_stored",
            symbol=symbol,
            date=date.isoformat(),
            rows=len(data),
            path=str(path)
        )
    
    async def read_tick_data(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime
    ) -> pl.DataFrame:
        """Tick adatok olvas√°sa d√°tumtartom√°nyb√≥l."""
        paths = []
        
        # √ñsszes relev√°ns f√°jl megtal√°l√°sa
        current_date = start_date
        while current_date <= end_date:
            path = self._get_path(symbol, current_date)
            if path.exists():
                paths.append(path)
            current_date += timedelta(days=1)
        
        if not paths:
            logger.warning(
                "no_data_found",
                symbol=symbol,
                start_date=start_date.isoformat(),
                end_date=end_date.isoformat()
            )
            return pl.DataFrame()
        
        # Adatok bet√∂lt√©se p√°rhuzamosan
        dfs = await asyncio.gather(*[
            self._read_parquet_async(path)
            for path in paths
        ])
        
        # √ñsszef≈±z√©s
        result = pl.concat(dfs)
        
        # D√°tum szerinti sz≈±r√©s (pontosabb)
        result = result.filter(
            (pl.col("timestamp") >= start_date) &
            (pl.col("timestamp") <= end_date)
        )
        
        logger.info(
            "tick_data_loaded",
            symbol=symbol,
            rows=len(result),
            start_date=start_date.isoformat(),
            end_date=end_date.isoformat()
        )
        
        return result
    
    async def _read_parquet_async(self, path: Path) -> pl.DataFrame:
        """Aszinkron Parquet olvas√°s."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            pl.read_parquet,
            path
        )
    
    async def get_available_dates(
        self,
        symbol: str
    ) -> List[datetime]:
        """El√©rhet≈ë d√°tumok lek√©rdez√©se."""
        symbol_path = self.BASE_PATH / symbol / "tick"
        
        if not symbol_path.exists():
            return []
        
        dates = []
        for year_dir in symbol_path.glob("year=*"):
            year = int(year_dir.name.split("=")[1])
            for month_dir in year_dir.glob("month=*"):
                month = int(month_dir.name.split("=")[1])
                for day_dir in month_dir.glob("day=*"):
                    day = int(day_dir.name.split("=")[1])
                    dates.append(datetime(year, month, day))
        
        return sorted(dates)
```

---

## üîÑ Resampler Service

### Tick -> OHLCV Konverzi√≥

```python
import polars as pl
from datetime import datetime, timedelta

class ResamplerService:
    """Tick adatok √°talak√≠t√°sa OHLCV form√°tumba."""
    
    def __init__(self, storage: ParquetStorageService):
        self.storage = storage
    
    async def resample_to_ohlcv(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        timeframe: str = "1m"
    ) -> pl.DataFrame:
        """Tick -> OHLCV √°talak√≠t√°s."""
        # Tick adatok bet√∂lt√©se
        ticks = await self.storage.read_tick_data(
            symbol,
            start_date,
            end_date
        )
        
        if len(ticks) == 0:
            return pl.DataFrame()
        
        # K√∂z√©p√°r sz√°m√≠t√°sa
        ticks = ticks.with_columns(
            mid_price=(pl.col("bid") + pl.col("ask")) / 2
        )
        
        # Resampling id≈ëalap√∫ ablakokkal
        if timeframe == "1m":
            rule = "1m"
        elif timeframe == "5m":
            rule = "5m"
        elif timeframe == "1h":
            rule = "1h"
        elif timeframe == "1d":
            rule = "1d"
        else:
            raise ValueError(f"Unsupported timeframe: {timeframe}")
        
        # OHLCV aggreg√°ci√≥
        ohlcv = ticks.group_by_dynamic(
            "timestamp",
            every=rule,
            closed="left"
        ).agg([
            pl.col("mid_price").first().alias("open"),
            pl.col("mid_price").max().alias("high"),
            pl.col("mid_price").min().alias("low"),
            pl.col("mid_price").last().alias("close"),
            pl.col("volume").sum().alias("volume"),
        ])
        
        logger.info(
            "resample_completed",
            symbol=symbol,
            timeframe=timeframe,
            input_rows=len(ticks),
            output_rows=len(ohlcv)
        )
        
        return ohlcv
    
    async def resample_for_vectorbt(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        timeframes: List[str] = ["1m", "5m", "1h"]
    ) -> Dict[str, pl.DataFrame]:
        """T√∂bb id≈ëkeretre val√≥ resampling VectorBT sz√°m√°ra."""
        results = {}
        
        for tf in timeframes:
            results[tf] = await self.resample_to_ohlcv(
                symbol,
                start_date,
                end_date,
                tf
            )
        
        return results
```

---

## üîç Adatmin≈ës√©g √©s Valid√°ci√≥

### Data Quality Checks

```python
class DataQualityService:
    """Adatmin≈ës√©g ellen≈ërz≈ë szolg√°ltat√°s."""
    
    @staticmethod
    def validate_tick_data(df: pl.DataFrame) -> Dict[str, Any]:
        """Tick adatok valid√°l√°sa."""
        report = {
            "total_rows": len(df),
            "duplicates": 0,
            "nulls": {},
            "outliers": {},
            "gaps": []
        }
        
        # Duplik√°tumok ellen≈ërz√©se
        duplicates = df.select(
            pl.col("timestamp").is_duplicated().sum()
        ).item()
        report["duplicates"] = duplicates
        
        # Null √©rt√©kek ellen≈ërz√©se
        for col in ["bid", "ask", "timestamp"]:
            null_count = df.select(
                pl.col(col).is_null().sum()
            ).item()
            report["nulls"][col] = null_count
        
        # Outlier detekt√°l√°s (3 szigma szab√°ly)
        if len(df) > 0:
            bid_mean = df.select(pl.col("bid").mean()).item()
            bid_std = df.select(pl.col("bid").std()).item()
            
            outliers = df.filter(
                (pl.col("bid") > bid_mean + 3 * bid_std) |
                (pl.col("bid") < bid_mean - 3 * bid_std)
            )
            report["outliers"]["bid"] = len(outliers)
        
        # Id≈ëbeli h√©zagok ellen≈ërz√©se
        if len(df) > 1:
            df_sorted = df.sort("timestamp")
            time_diffs = df_sorted.select(
                pl.col("timestamp").diff().alias("diff")
            )
            
            # T√∫l nagy id≈ëbeli k√ºl√∂nbs√©gek
            gaps = time_diffs.filter(
                pl.col("diff") > pl.duration(minutes=5)
            )
            report["gaps"] = gaps.select("timestamp").to_series().to_list()
        
        return report
    
    @staticmethod
    def clean_tick_data(df: pl.DataFrame) -> pl.DataFrame:
        """Tick adatok tiszt√≠t√°sa."""
        # Duplik√°tumok elt√°vol√≠t√°sa
        df = df.unique(subset=["timestamp"], keep="first")
        
        # Null √©rt√©kek elt√°vol√≠t√°sa
        df = df.drop_nulls(subset=["bid", "ask", "timestamp"])
        
        # Rendez√©s id≈ë szerint
        df = df.sort("timestamp")
        
        return df
```

---

## üìä Teljes√≠tm√©ny Optimaliz√°ci√≥

### Chunking √©s Streamel√©s

```python
class ChunkedStorageService:
    """Nagy adathalmazok chunkolva t√∂rt√©n≈ë t√°rol√°sa."""
    
    CHUNK_SIZE = 100_000  # 100k tick per chunk
    
    async def store_large_tick_dataset(
        self,
        symbol: str,
        data: pl.DataFrame,
        start_date: datetime
    ) -> None:
        """Nagy adathalmaz t√°rol√°sa chunkokban."""
        total_rows = len(data)
        num_chunks = (total_rows // self.CHUNK_SIZE) + 1
        
        logger.info(
            "large_dataset_storage_started",
            symbol=symbol,
            total_rows=total_rows,
            chunk_size=self.CHUNK_SIZE,
            num_chunks=num_chunks
        )
        
        for i in range(num_chunks):
            start_idx = i * self.CHUNK_SIZE
            end_idx = min((i + 1) * self.CHUNK_SIZE, total_rows)
            
            chunk = data[start_idx:end_idx]
            
            # D√°tum meghat√°roz√°sa a chunk els≈ë elem√©b≈ël
            chunk_date = chunk[0, "timestamp"]
            
            await self.storage.store_tick_data(
                symbol,
                chunk,
                chunk_date
            )
            
            logger.debug(
                "chunk_stored",
                chunk_index=i,
                rows=len(chunk)
            )
        
        logger.info(
            "large_dataset_storage_completed",
            symbol=symbol,
            total_rows=total_rows
        )
```

### Predicate Pushdown

```python
# Polars automatikusan alkalmazza a predicate pushdown-ot
# Csak a sz√ºks√©ges part√≠ci√≥k √©s sorok lesznek bet√∂ltve

# P√©lda: Gyors lek√©rdez√©s csak bizonyos √≥r√°kra
morning_ticks = await storage.read_tick_data(
    symbol="EURUSD",
    start_date=datetime(2023, 12, 23, 8, 0),
    end_date=datetime(2023, 12, 23, 12, 0)
)

# Polars csak a 2023-12-23 nap adatait t√∂lti be
# √©s ut√°na sz≈±ri az id≈ëintervallumra
```

---

## üîê Biztons√°g √©s Integrit√°s

### Adatbiztons√°g

```python
import hashlib

class DataIntegrityService:
    """Adatintegrit√°s ellen≈ërz≈ë."""
    
    @staticmethod
    def calculate_checksum(df: pl.DataFrame) -> str:
        """DataFrame checksum sz√°m√≠t√°sa."""
        # Csak a fontos oszlopok alapj√°n
        data_str = df.select(["timestamp", "bid", "ask"]).to_csv()
        return hashlib.sha256(data_str.encode()).hexdigest()
    
    @staticmethod
    async def verify_data_integrity(
        symbol: str,
        date: datetime
    ) -> bool:
        """Adatintegrit√°s ellen≈ërz√©se."""
        path = storage._get_path(symbol, date)
        
        if not path.exists():
            return False
        
        try:
            # Parquet f√°jl ellen≈ërz√©se
            df = pl.read_parquet(path)
            
            # Alapvet≈ë ellen≈ërz√©sek
            assert len(df) > 0, "Empty dataframe"
            assert "timestamp" in df.columns
            assert "bid" in df.columns
            assert "ask" in df.columns
            
            # Rendez√©s ellen≈ërz√©se
            assert df["timestamp"].is_sorted(), "Data not sorted"
            
            logger.info(
                "data_integrity_verified",
                symbol=symbol,
                date=date.isoformat(),
                rows=len(df)
            )
            
            return True
            
        except Exception as e:
            logger.error(
                "data_integrity_check_failed",
                symbol=symbol,
                date=date.isoformat(),
                error=str(e)
            )
            return False
```

---

## üìã K√∂vetkez≈ë L√©p√©sek

1. **Collectorok:** L√°sd [`05_collectors_strategy.md`](05_collectors_strategy.md)

---

## üîó Kapcsol√≥d√≥ Dokumentumok

- [Rendszerarchitekt√∫ra](01_system_architecture.md)
- [Dinamikus Konfigur√°ci√≥](02_dynamic_configuration.md)
- [Megfigyelhet≈ës√©g](03_observability_logging.md)
- [Fejleszt√©si √ötmutat√≥](docs/development/unified_development_guide.md)